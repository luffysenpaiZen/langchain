{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d5df4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4608022c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: 'Hello, how are you doing today?'\n",
      "Input IDs: tensor([[ 101, 7592, 1010, 2129, 2024, 2017, 2725, 2651, 1029,  102]])\n",
      "Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "Decoded Tokens: ['[CLS]', 'hello', ',', 'how', 'are', 'you', 'doing', 'today', '?', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "sentence = \"Hello, how are you doing today?\"\n",
    "encoded_input = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "print(f\"Original sentence: '{sentence}'\")\n",
    "print(f\"Input IDs: {encoded_input['input_ids']}\")\n",
    "print(f\"Attention Mask: {encoded_input['attention_mask']}\")\n",
    "\n",
    "decoded_tokens = tokenizer.convert_ids_to_tokens(encoded_input['input_ids'][0])\n",
    "print(f\"Decoded Tokens: {decoded_tokens}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40b80238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: 'This movie is very bad, I hate it.'\n",
      "Predicted Label: NEGATIVE\n",
      "Confidence Score: 0.9998\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "#Define a model name and load both tokenizer and model\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Text for analysis\n",
    "text_negative = 'This movie is very bad, I hate it.'\n",
    "\n",
    "#Tokenize the input text\n",
    "# The tokenizer must be the one corresponding to the model\n",
    "negative_token = tokenizer(text_negative, return_tensors='pt')\n",
    "\n",
    "#Perform inference\n",
    "# Call the loaded 'model' object, not the class\n",
    "with torch.no_grad():\n",
    "    output = model(**negative_token)\n",
    "\n",
    "#Get predictions and confidence score\n",
    "predictions = torch.argmax(output.logits, dim=-1)\n",
    "\n",
    "#Access the config from the loaded 'model' object\n",
    "prediction_label = model.config.id2label[predictions.item()]\n",
    "\n",
    "confidence_score = torch.softmax(output.logits, dim=-1)[0][predictions.item()].item() # type: ignore\n",
    "\n",
    "#Print the results\n",
    "print(f\"Text: '{text_negative}'\")\n",
    "print(f\"Predicted Label: {prediction_label}\")\n",
    "print(f\"Confidence Score: {confidence_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89984882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8a6303e3e4a42a08879dd684cb056bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/953 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Issac Chowdary\\Downloads\\langchain_yt\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Issac Chowdary\\.cache\\huggingface\\hub\\models--nlptown--bert-base-multilingual-uncased-sentiment. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d302afa68452477eb6ce48ad250a45a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/669M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aaa62b352f249a080e2676bd9519914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/39.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "760b74dd1bb34534928e67754f1bb6cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af64cbefcefb4092b1996c17a146ab8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': '5 stars', 'score': 0.8002934455871582}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline #type: ignore\n",
    "\n",
    "classifier=pipeline('text-classification',model='nlptown/bert-base-multilingual-uncased-sentiment')\n",
    "print(classifier(\"i am so happy today\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e12df99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to google-t5/t5-base and revision a9723ea (https://huggingface.co/google-t5/t5-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'translation_text': 'hi qui Ãªtes-vous?'}]\n"
     ]
    }
   ],
   "source": [
    "translate=pipeline('translation_en_to_fr') # type: ignore\n",
    "print(translate('hi who are you?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb48c3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: hugging face makes nlp easy for everyone\n",
      "Input IDs: tensor([[  101, 17662,  2227,  3084, 17953,  2361,  3733,  2005,  3071,   102]])\n",
      "Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "Decoded Tokens (for understanding): [CLS] hugging face makes nlp easy for everyone [SEP]\n",
      "Tokens (raw): ['hugging', 'face', 'makes', 'nl', '##p', 'easy', 'for', 'everyone']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer=AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "text='hugging face makes nlp easy for everyone'\n",
    "\n",
    "encoded_input=tokenizer(text,padding=True,truncation=True,return_tensors='pt')\n",
    "\n",
    "print(\"Original Text:\", text)\n",
    "print(\"Input IDs:\", encoded_input[\"input_ids\"])\n",
    "print(\"Attention Mask:\", encoded_input[\"attention_mask\"])\n",
    "print(\"Decoded Tokens (for understanding):\",tokenizer.decode(encoded_input[\"input_ids\"][0]))\n",
    "print(\"Tokens (raw):\", tokenizer.tokenize (text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e76da960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch\n",
    "\n",
    "sentence=[\n",
    "    'this is a short sentence',\n",
    "    'hugging face api is very good',\n",
    "    'hugging face makes nlp easy for everyone'\n",
    "]\n",
    "\n",
    "encoded_sentences=tokenizer(sentence,padding=True,truncation=True,return_tensors='pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22ba6e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encoded Sentences (with padding & truncation):\n",
      "Input IDs shape: torch.Size([3, 10])\n",
      "Input IDs for sentences: tensor([[  101,  2023,  2003,  1037,  2460,  6251,   102,     0,     0,     0],\n",
      "        [  101, 17662,  2227, 17928,  2003,  2200,  2204,   102,     0,     0],\n",
      "        [  101, 17662,  2227,  3084, 17953,  2361,  3733,  2005,  3071,   102]])\n",
      "Attention Mask for sentences: tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "Input IDs for sentences: tensor([[  101,  2023,  2003,  1037,  2460,  6251,   102,     0,     0,     0],\n",
      "        [  101, 17662,  2227, 17928,  2003,  2200,  2204,   102,     0,     0],\n",
      "        [  101, 17662,  2227,  3084, 17953,  2361,  3733,  2005,  3071,   102]])\n",
      "Attention Mask for sentences: tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nEncoded Sentences (with padding & truncation):\")\n",
    "print(\"Input IDs shape:\", encoded_sentences[\"input_ids\"].shape)\n",
    "print(\"Input IDs for sentences:\", encoded_sentences[\"input_ids\"])\n",
    "print(\"Attention Mask for sentences:\", encoded_sentences[\"attention_mask\"])\n",
    "print(\"Input IDs for sentences:\", encoded_sentences[\"input_ids\"])\n",
    "print(\"Attention Mask for sentences:\", encoded_sentences[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69be36a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f55082ba93ab41ea9d3fd7da9b9ec8d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Issac Chowdary\\Downloads\\langchain_yt\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Issac Chowdary\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial vocabulary size: 30522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New vocabulary size: 30525\n"
     ]
    }
   ],
   "source": [
    "# custome tokenizer\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model=AutoModelForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Check initial vocabulary size\n",
    "print(f\"Initial vocabulary size: {len(tokenizer)}\")\n",
    "# Define new tokens specific to a medical domain\n",
    "new_medical_tokens = [\"MyocardialInfarction\", \"HypertensionCrisis\", \"[PHI]\"]\n",
    "# Add new tokens to the tokenizer\n",
    "tokenizer.add_tokens(new_medical_tokens)\n",
    "# Resize the model's token embeddings to account for new tokens\n",
    "model.resize_token_embeddings (len(tokenizer))\n",
    "print(f\"New vocabulary size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0905963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text with new medical terms: Patient presents with MyocardialInfarction and a severe Hypertension Crisis.\n",
      "Encoded Input IDs with new tokens: tensor([[  101,  5776,  7534,  2007, 30522,  1998,  1037,  5729, 23760, 29048,\n",
      "          5325,  1012,   102]])\n",
      "Decoded (to see if new tokens are recognized): [CLS] patient presents with MyocardialInfarction and a severe hypertension crisis. [SEP]\n"
     ]
    }
   ],
   "source": [
    "#Test the new tokenizer\n",
    "text_with_medical_term = \"Patient presents with MyocardialInfarction and a severe Hypertension Crisis.\"\n",
    "encoded_custom = tokenizer(text_with_medical_term, return_tensors=\"pt\")\n",
    "print(\"\\nText with new medical terms:\", text_with_medical_term)\n",
    "print(\"Encoded Input IDs with new tokens:\", encoded_custom[\"input_ids\"])\n",
    "print(\"Decoded (to see if new tokens are recognized):\", tokenizer.decode(encoded_custom[\"input_ids\"][0]))\n",
    "#Verify that the new tokens are treated as single units\n",
    "# If they were split, the decoded output would show subwords or special tokens like ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c670004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset features: {'text': Value('string'), 'label': ClassLabel(names=['neg', 'pos'])}\n",
      "First example: {'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.', 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "#1. Load a dataset from the Hugging Face Hub (e.g., IMDB for sentiment analysis)\n",
    "# For a local CSV, you'd use: load_dataset('csv', data_files='your_reviews.csv')\n",
    "dataset = load_dataset(\"imdb\", split=\"train[:1000]\") # Load a small subset for demonstration\n",
    "print(f\"Dataset features: {dataset.features}\")\n",
    "print(f\"First example: {dataset[0]}\")\n",
    "#2. Load a tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56e61a15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f48dcfbea7d4a1cb151cdea7402ccf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 1000\n",
      "})\n",
      "[101, 1045, 12524, 1045, 2572, 8025, 1011, 3756, 2013, 2026]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'],padding='max_length',truncation=True)\n",
    "\n",
    "tokenized_dataset=dataset.map(tokenize_function,batched=True)\n",
    "print(tokenized_dataset)\n",
    "print(tokenized_dataset[0]['input_ids'][:10]) # type: ignore\n",
    "print(tokenized_dataset[0]['attention_mask'][:10]) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6526d602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final processed dataset features: {'labels': ClassLabel(names=['neg', 'pos']), 'input_ids': List(Value('int32')), 'token_type_ids': List(Value('int8')), 'attention_mask': List(Value('int8'))}\n",
      "First processed example (PyTorch tensor format):\n",
      "torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "#5. Rename and select columns to match model expectations (e.g., 'label' to 'labels')\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"text\"]) # Remove original text column\n",
    "tokenized_dataset.set_format(\"torch\") # Set format for PyTorch training\n",
    "print(\"\\nFinal processed dataset features:\", tokenized_dataset.features)\n",
    "print(\"First processed example (PyTorch tensor format):\")\n",
    "print(tokenized_dataset[0][\"input_ids\"].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
